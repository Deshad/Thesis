{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initial System Status ===\n",
      "Total RAM: 15.7 GB\n",
      "Available RAM: 3.7 GB\n",
      "Used RAM: 12.0 GB (76.6%)\n",
      "Process RSS: 0.26 GB\n",
      "Process VMS: 0.60 GB\n",
      "‚ÑπÔ∏è  Memory usage is getting high, monitoring recommended\n",
      "üîß Reducing batch size for low available memory\n",
      "\n",
      "=== Starting Training Pipeline ===\n",
      "Starting Elite Mini 800 G9 optimized pipeline...\n",
      "Loading data information...\n",
      "Searching for data in: C:\\Users\\2955352g\\Desktop\\pig_data_edinburgh\n",
      "Files found in data directory:\n",
      "  test_video.mp4 (.mp4)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  output.json (.json)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  output.json (.json)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  output.json (.json)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  output.json (.json)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  output.json (.json)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  output.json (.json)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  output.json (.json)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  output.json (.json)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  output.json (.json)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  output.json (.json)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  output.json (.json)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  output.json (.json)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "  background.png (.png)\n",
      "  background_depth.png (.png)\n",
      "  color.mp4 (.mp4)\n",
      "  depth.mp4 (.mp4)\n",
      "  depth_scale.npy (.npy)\n",
      "  inverse_intrinsic.npy (.npy)\n",
      "  mask.png (.png)\n",
      "  rot.npy (.npy)\n",
      "  times.txt (.txt)\n",
      "\n",
      "Found 225 video files:\n",
      "  test_video.mp4\n",
      "  color.mp4\n",
      "  depth.mp4\n",
      "  color.mp4\n",
      "  depth.mp4\n",
      "  ... and 220 more\n",
      "\n",
      "No annotation files found. Creating basic structure from video files...\n",
      "Creating basic annotations from video files...\n",
      "Created 600 basic annotations\n",
      "‚ö†Ô∏è  WARNING: Using dummy bounding boxes! You need real annotations for proper training.\n",
      "Using 600 frames for training\n",
      "Preparing dataset...\n",
      "Processing 600 frames...\n",
      "Processed 0/600 frames\n",
      "Processed 200/600 frames\n",
      "Processed 400/600 frames\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21.5M/21.5M [00:29<00:00, 755kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training detector (estimated 2-4 hours)...\n",
      "Training with Elite Mini 800 G9 optimizations (16GB RAM)...\n",
      "Ultralytics 8.3.166  Python-3.11.13 torch-2.5.1 CPU (12th Gen Intel Core(TM) i5-12500T)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=2, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=elite_mini_dataset/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=7, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train, save_frames=False, save_json=False, save_period=10, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to 'C:\\Users\\2955352g\\AppData\\Roaming\\Ultralytics\\Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00<00:00, 5.18MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 129 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 14.710.8 MB/s, size: 90.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\2955352g\\Desktop\\Thesis\\Codes\\elite_mini_dataset\\labels... 180 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:00<00:00, 716.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\2955352g\\Desktop\\Thesis\\Codes\\elite_mini_dataset\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB RAM): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:00<00:00, 1771.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 1398.7626.8 MB/s, size: 140.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\2955352g\\Desktop\\Thesis\\Codes\\elite_mini_dataset\\labels.cache... 180 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.2GB RAM): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:00<00:00, 1769.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30         0G      2.484      3.498      2.529          5        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:39<00:00,  1.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:35<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180    0.00338      0.783     0.0032   0.000506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30         0G      2.476      3.318      2.419          2        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:45<00:00,  1.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:33<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30         0G      2.448      3.236      2.373          3        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:42<00:00,  1.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:32<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30         0G      2.442      3.035      2.356          4        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:40<00:00,  1.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:39<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30         0G      2.409      3.071      2.335          3        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:45<00:00,  1.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:34<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180   0.000591      0.161   0.000344   3.44e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30         0G      2.297      2.807      2.245          3        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:44<00:00,  1.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:36<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180     0.0843      0.611     0.0578     0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30         0G      2.024      2.706      2.111          2        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:38<00:00,  1.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:29<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180     0.0438      0.667      0.165     0.0511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30         0G      2.001      2.553      2.056          3        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:21<00:00,  1.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:29<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180     0.0595      0.244     0.0809     0.0222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/30         0G      1.883      2.444      1.969          4        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:28<00:00,  1.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:31<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.288      0.572      0.262      0.122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/30         0G      1.905      2.438      1.968          2        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:24<00:00,  1.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:29<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180       0.44      0.611      0.423      0.265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/30         0G      1.812      2.395      1.906          2        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:23<00:00,  1.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:29<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180     0.0566      0.667      0.164      0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/30         0G      1.756      2.304      1.882          6        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:21<00:00,  1.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:29<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.354        0.5      0.278      0.108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/30         0G      1.815      2.327      1.962          2        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:23<00:00,  1.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:29<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.203      0.578       0.16     0.0459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/30         0G      1.774       2.22      1.896          4        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:22<00:00,  1.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:39<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.418      0.533      0.397      0.113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/30         0G      1.702      2.243      1.841          3        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [02:02<00:00,  1.36s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:38<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.467      0.544      0.364      0.261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/30         0G      1.697      2.058      1.833          3        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:21<00:00,  1.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:29<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.528      0.611      0.565      0.309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/30         0G      1.711      2.168      1.864          2        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:21<00:00,  1.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:29<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.619      0.583      0.549      0.282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/30         0G      1.574      1.957      1.747          5        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:22<00:00,  1.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:28<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.861      0.567       0.65      0.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/30         0G      1.455      1.927       1.67          5        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:21<00:00,  1.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:29<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.991      0.593      0.752      0.524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/30         0G        1.5      1.953      1.645          6        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:21<00:00,  1.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:29<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.941      0.633      0.817      0.578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/30         0G      1.362      1.973      1.589          2        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:20<00:00,  1.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:28<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.795      0.712      0.832      0.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/30         0G      1.317      1.693      1.502          2        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:22<00:00,  1.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:29<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.711      0.628       0.74      0.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/30         0G      1.284      1.617        1.5          2        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:20<00:00,  1.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:28<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.833      0.578       0.66      0.443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/30         0G      1.221      1.539      1.447          2        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:20<00:00,  1.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:28<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.891      0.589      0.763      0.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/30         0G        1.2      1.467      1.451          2        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:20<00:00,  1.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:28<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.261      0.645      0.288      0.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/30         0G      1.166      1.408      1.405          2        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:20<00:00,  1.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:28<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.785      0.628      0.669      0.455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/30         0G      1.154      1.369      1.423          2        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [01:20<00:00,  1.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:28<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.637      0.772      0.673      0.427\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 7 epochs. Best results observed at epoch 20, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=7) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "27 epochs completed in 0.905 hours.\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.166  Python-3.11.13 torch-2.5.1 CPU (12th Gen Intel Core(TM) i5-12500T)\n",
      "Model summary (fused): 72 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:27<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180        180      0.941      0.633      0.817      0.578\n",
      "Speed: 0.7ms preprocess, 146.5ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Training completed in 0.92 hours\n",
      "Training completed successfully!\n",
      "\n",
      "=== Post-Training Memory Status ===\n",
      "Total RAM: 15.7 GB\n",
      "Available RAM: 4.4 GB\n",
      "Used RAM: 11.2 GB (71.7%)\n",
      "Process RSS: 2.01 GB\n",
      "Process VMS: 2.75 GB\n",
      "‚ÑπÔ∏è  Memory usage is getting high, monitoring recommended\n",
      "\n",
      "=== Final Memory Status ===\n",
      "Total RAM: 15.7 GB\n",
      "Available RAM: 4.4 GB\n",
      "Used RAM: 11.2 GB (71.7%)\n",
      "Process RSS: 2.01 GB\n",
      "Process VMS: 2.75 GB\n",
      "‚ÑπÔ∏è  Memory usage is getting high, monitoring recommended\n"
     ]
    }
   ],
   "source": [
    "# Optimizations for HP Mini\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Optimized configurations for HP Elite Mini 800 G9 (i5-12500T) with 16GB RAM\n",
    "ELITE_MINI_CONFIG = {\n",
    "    'yolo_model': 'yolov8s.pt',  # Small model (good balance)\n",
    "    'img_size': 640,  # Standard size\n",
    "    'batch_size': 6,  # Increased batch size for 16GB RAM\n",
    "    'epochs': 30,  # More epochs with sufficient RAM\n",
    "    'patience': 7,  # Longer patience\n",
    "    'target_size': (224, 224),  # Standard target size\n",
    "    'frame_skip': 3,  # Process more frames\n",
    "    'max_frames': 8000,  # Handle more frames with 16GB\n",
    "    'workers': 8,  # More workers for faster data loading\n",
    "    'cache_ram': True,  # Enable RAM caching\n",
    "}\n",
    "\n",
    "class OptimizedDataGenerator:\n",
    "    \"\"\"Memory-efficient data generator for HP Mini\"\"\"\n",
    "    \n",
    "    def __init__(self, data_info, config=ELITE_MINI_CONFIG):  # Fixed: Changed HP_MINI_CONFIG to ELITE_MINI_CONFIG\n",
    "        self.data_info = data_info[:config['max_frames']]  # Limit data\n",
    "        self.config = config\n",
    "        self.current_idx = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.current_idx >= len(self.data_info):\n",
    "            raise StopIteration\n",
    "            \n",
    "        data = self.data_info.iloc[self.current_idx]\n",
    "        self.current_idx += 1\n",
    "        \n",
    "        try:\n",
    "            # Load and process frame\n",
    "            cap = cv2.VideoCapture(str(data['video_path']))\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, data['frame_num'])\n",
    "            ret, frame = cap.read()\n",
    "            cap.release()\n",
    "            \n",
    "            if ret:\n",
    "                # Resize to smaller size\n",
    "                frame = cv2.resize(frame, self.config['target_size'])\n",
    "                frame = frame.astype(np.float32) / 255.0\n",
    "                return frame, data['behavior_encoded']\n",
    "            else:\n",
    "                return self.__next__()  # Skip failed frames\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading frame: {e}\")\n",
    "            return self.__next__()\n",
    "\n",
    "class EliteMiniYOLODetector:\n",
    "    \"\"\"Optimized YOLO detector for HP Elite Mini 800 G9\"\"\"\n",
    "    \n",
    "    def __init__(self, config=ELITE_MINI_CONFIG):\n",
    "        self.config = config\n",
    "        self.model = YOLO(config['yolo_model'])\n",
    "        \n",
    "    def train_optimized(self, data_yaml):\n",
    "        \"\"\"Train with Elite Mini optimizations\"\"\"\n",
    "        print(\"Training with Elite Mini 800 G9 optimizations (16GB RAM)...\")\n",
    "        \n",
    "        results = self.model.train(\n",
    "            data=data_yaml,\n",
    "            epochs=self.config['epochs'],\n",
    "            imgsz=self.config['img_size'],\n",
    "            batch=self.config['batch_size'],\n",
    "            device='cpu',\n",
    "            workers=self.config['workers'],\n",
    "            patience=self.config['patience'],\n",
    "            cache=self.config['cache_ram'],  # Enable RAM caching\n",
    "            save_period=10,\n",
    "            verbose=True,\n",
    "            amp=False,  # Keep disabled for CPU\n",
    "            optimizer='AdamW',\n",
    "            lr0=0.01,\n",
    "            weight_decay=0.0005,\n",
    "            momentum=0.937,\n",
    "        )\n",
    "        return results\n",
    "    \n",
    "    def detect_optimized(self, frame, conf=0.3):\n",
    "        \"\"\"Optimized detection for HP Mini\"\"\"\n",
    "        # Resize frame to smaller size\n",
    "        small_frame = cv2.resize(frame, (self.config['img_size'], self.config['img_size']))\n",
    "        \n",
    "        # Run inference\n",
    "        results = self.model.predict(\n",
    "            small_frame,\n",
    "            conf=conf,\n",
    "            device='cpu',\n",
    "            verbose=False,\n",
    "            half=False,  # Disable half precision\n",
    "        )\n",
    "        \n",
    "        # Scale detections back to original size\n",
    "        h, w = frame.shape[:2]\n",
    "        scale_x = w / self.config['img_size']\n",
    "        scale_y = h / self.config['img_size']\n",
    "        \n",
    "        detections = []\n",
    "        for result in results:\n",
    "            if result.boxes is not None:\n",
    "                boxes = result.boxes.xyxy.cpu().numpy()\n",
    "                confs = result.boxes.conf.cpu().numpy()\n",
    "                \n",
    "                for box, conf in zip(boxes, confs):\n",
    "                    # Scale back to original size\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    x1, x2 = int(x1 * scale_x), int(x2 * scale_x)\n",
    "                    y1, y2 = int(y1 * scale_y), int(y2 * scale_y)\n",
    "                    \n",
    "                    detections.append({\n",
    "                        'bbox': [x1, y1, x2-x1, y2-y1],\n",
    "                        'confidence': float(conf)\n",
    "                    })\n",
    "        \n",
    "        return detections\n",
    "\n",
    "def prepare_standard_dataset(data_info, output_dir, config=ELITE_MINI_CONFIG):\n",
    "    \"\"\"Prepare standard dataset for Elite Mini\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    (output_dir / 'images').mkdir(parents=True, exist_ok=True)\n",
    "    (output_dir / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Can handle more frames\n",
    "    limited_data = data_info.head(config['max_frames'])\n",
    "    \n",
    "    # Create data.yaml\n",
    "    data_yaml = {\n",
    "        'path': str(output_dir.absolute()),\n",
    "        'train': 'images',\n",
    "        'val': 'images',\n",
    "        'names': ['pig'],\n",
    "        'nc': 1\n",
    "    }\n",
    "    \n",
    "    import yaml\n",
    "    with open(output_dir / 'data.yaml', 'w') as f:\n",
    "        yaml.dump(data_yaml, f)\n",
    "    \n",
    "    print(f\"Processing {len(limited_data)} frames...\")\n",
    "    \n",
    "    for idx, row in limited_data.iterrows():\n",
    "        if idx % 200 == 0:\n",
    "            print(f\"Processed {idx}/{len(limited_data)} frames\")\n",
    "            gc.collect()\n",
    "        \n",
    "        try:\n",
    "            # Load frame\n",
    "            cap = cv2.VideoCapture(str(row['video_path']))\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, row['frame_num'])\n",
    "            ret, frame = cap.read()\n",
    "            cap.release()\n",
    "            \n",
    "            if not ret:\n",
    "                continue\n",
    "            \n",
    "            # Keep original size for better quality\n",
    "            frame = cv2.resize(frame, (config['img_size'], config['img_size']))\n",
    "            \n",
    "            # Save image with good quality\n",
    "            img_path = output_dir / 'images' / f\"{row['video_id']}_{row['frame_num']}.jpg\"\n",
    "            cv2.imwrite(str(img_path), frame, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
    "            \n",
    "            # Process bbox (same as before)\n",
    "            bbox = row['bbox']\n",
    "            if isinstance(bbox, str):\n",
    "                import ast\n",
    "                bbox = ast.literal_eval(bbox)\n",
    "            if isinstance(bbox, dict):\n",
    "                x = bbox.get('x', 0)\n",
    "                y = bbox.get('y', 0)\n",
    "                w = bbox.get('width', bbox.get('w', 100))\n",
    "                h = bbox.get('height', bbox.get('h', 100))\n",
    "                bbox = [x, y, w, h]\n",
    "            \n",
    "            # Convert to YOLO format\n",
    "            img_h, img_w = config['img_size'], config['img_size']\n",
    "            x_center = (bbox[0] + bbox[2]/2) / img_w\n",
    "            y_center = (bbox[1] + bbox[3]/2) / img_h\n",
    "            width = bbox[2] / img_w\n",
    "            height = bbox[3] / img_h\n",
    "            \n",
    "            # Clamp values\n",
    "            x_center = max(0, min(1, x_center))\n",
    "            y_center = max(0, min(1, y_center))\n",
    "            width = max(0, min(1, width))\n",
    "            height = max(0, min(1, height))\n",
    "            \n",
    "            # Save label\n",
    "            label_path = output_dir / 'labels' / f\"{row['video_id']}_{row['frame_num']}.txt\"\n",
    "            with open(label_path, 'w') as f:\n",
    "                f.write(f\"0 {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "def load_annotated_data_info(data_root=\"C:/Users/2955352g/Desktop/pig_data_edinburgh\"):\n",
    "    \"\"\"Load annotated data from various possible formats\"\"\"\n",
    "    data_root = Path(data_root)\n",
    "    \n",
    "    print(f\"Searching for data in: {data_root}\")\n",
    "    \n",
    "    # Check what files exist in the data directory\n",
    "    if not data_root.exists():\n",
    "        print(f\"Data directory {data_root} does not exist!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # List all files in the directory\n",
    "    print(\"Files found in data directory:\")\n",
    "    for item in data_root.rglob(\"*\"):\n",
    "        if item.is_file():\n",
    "            print(f\"  {item.name} ({item.suffix})\")\n",
    "    \n",
    "    # Try to load from common annotation formats\n",
    "    annotation_data = []\n",
    "    \n",
    "    # Method 1: Try to load from CSV files\n",
    "    csv_files = list(data_root.glob(\"*.csv\"))\n",
    "    if csv_files:\n",
    "        print(f\"\\nFound {len(csv_files)} CSV files:\")\n",
    "        for csv_file in csv_files:\n",
    "            print(f\"  {csv_file.name}\")\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                print(f\"    Columns: {list(df.columns)}\")\n",
    "                print(f\"    Shape: {df.shape}\")\n",
    "                annotation_data.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"    Error reading {csv_file}: {e}\")\n",
    "    \n",
    "    # Method 2: Try to load from JSON files\n",
    "    json_files = list(data_root.glob(\"*.json\"))\n",
    "    if json_files:\n",
    "        print(f\"\\nFound {len(json_files)} JSON files:\")\n",
    "        for json_file in json_files:\n",
    "            print(f\"  {json_file.name}\")\n",
    "            try:\n",
    "                with open(json_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                print(f\"    Keys: {list(data.keys()) if isinstance(data, dict) else 'List format'}\")\n",
    "                # Convert JSON to DataFrame format if needed\n",
    "                if isinstance(data, list):\n",
    "                    df = pd.DataFrame(data)\n",
    "                    annotation_data.append(df)\n",
    "                elif isinstance(data, dict):\n",
    "                    df = pd.DataFrame([data])\n",
    "                    annotation_data.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"    Error reading {json_file}: {e}\")\n",
    "    \n",
    "    # Method 3: Look for video files and create basic structure\n",
    "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv']\n",
    "    video_files = []\n",
    "    for ext in video_extensions:\n",
    "        video_files.extend(data_root.glob(f\"*{ext}\"))\n",
    "        video_files.extend(data_root.glob(f\"**/*{ext}\"))\n",
    "    \n",
    "    if video_files:\n",
    "        print(f\"\\nFound {len(video_files)} video files:\")\n",
    "        for video_file in video_files[:5]:  # Show first 5\n",
    "            print(f\"  {video_file.name}\")\n",
    "        if len(video_files) > 5:\n",
    "            print(f\"  ... and {len(video_files) - 5} more\")\n",
    "    \n",
    "    # If we found annotation data, try to standardize it\n",
    "    if annotation_data:\n",
    "        print(f\"\\nProcessing {len(annotation_data)} annotation files...\")\n",
    "        \n",
    "        # Combine all annotation data\n",
    "        combined_df = pd.concat(annotation_data, ignore_index=True)\n",
    "        \n",
    "        # Try to standardize column names\n",
    "        standardized_df = standardize_annotation_format(combined_df, data_root)\n",
    "        \n",
    "        if not standardized_df.empty:\n",
    "            print(f\"Successfully loaded {len(standardized_df)} annotations\")\n",
    "            return standardized_df\n",
    "    \n",
    "    # If no annotation files found, create a basic structure from video files\n",
    "    if video_files:\n",
    "        print(\"\\nNo annotation files found. Creating basic structure from video files...\")\n",
    "        return create_basic_annotations_from_videos(video_files)\n",
    "    \n",
    "    print(\"\\nNo suitable data found!\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def standardize_annotation_format(df, data_root):\n",
    "    \"\"\"Try to standardize annotation format to expected columns\"\"\"\n",
    "    print(\"Attempting to standardize annotation format...\")\n",
    "    print(f\"Input columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Common column name mappings\n",
    "    column_mappings = {\n",
    "        # Video path variations\n",
    "        'video_path': ['video_path', 'video_file', 'video', 'filename', 'file_path'],\n",
    "        'video_id': ['video_id', 'video_name', 'id', 'video', 'file_id'],\n",
    "        \n",
    "        # Frame number variations\n",
    "        'frame_num': ['frame_num', 'frame_number', 'frame', 'frame_id', 'timestamp'],\n",
    "        \n",
    "        # Bounding box variations\n",
    "        'bbox': ['bbox', 'bounding_box', 'box', 'coordinates', 'bounds'],\n",
    "        'x': ['x', 'x1', 'left', 'bbox_x'],\n",
    "        'y': ['y', 'y1', 'top', 'bbox_y'],\n",
    "        'width': ['width', 'w', 'bbox_width', 'bbox_w'],\n",
    "        'height': ['height', 'h', 'bbox_height', 'bbox_h'],\n",
    "        \n",
    "        # Behavior/label variations\n",
    "        'behavior': ['behavior', 'label', 'class', 'category', 'behavior_label'],\n",
    "        'behavior_encoded': ['behavior_encoded', 'label_encoded', 'class_id'],\n",
    "    }\n",
    "    \n",
    "    # Create new standardized dataframe\n",
    "    standardized_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Extract video information\n",
    "            video_path = None\n",
    "            video_id = None\n",
    "            \n",
    "            for col in df.columns:\n",
    "                if any(vid_col in col.lower() for vid_col in column_mappings['video_path']):\n",
    "                    video_path = row[col]\n",
    "                    break\n",
    "            \n",
    "            for col in df.columns:\n",
    "                if any(vid_col in col.lower() for vid_col in column_mappings['video_id']):\n",
    "                    video_id = row[col]\n",
    "                    break\n",
    "            \n",
    "            # If video_path is relative, make it absolute\n",
    "            if video_path and not os.path.isabs(video_path):\n",
    "                video_path = data_root / video_path\n",
    "            \n",
    "            # Extract frame number\n",
    "            frame_num = 0\n",
    "            for col in df.columns:\n",
    "                if any(frame_col in col.lower() for frame_col in column_mappings['frame_num']):\n",
    "                    frame_num = row[col]\n",
    "                    break\n",
    "            \n",
    "            # Extract bounding box\n",
    "            bbox = None\n",
    "            \n",
    "            # Try to find bbox as single column\n",
    "            for col in df.columns:\n",
    "                if any(bbox_col in col.lower() for bbox_col in column_mappings['bbox']):\n",
    "                    bbox = row[col]\n",
    "                    break\n",
    "            \n",
    "            # If no single bbox column, try to construct from x,y,w,h\n",
    "            if bbox is None:\n",
    "                x, y, w, h = None, None, None, None\n",
    "                \n",
    "                for col in df.columns:\n",
    "                    col_lower = col.lower()\n",
    "                    if any(x_col in col_lower for x_col in column_mappings['x']):\n",
    "                        x = row[col]\n",
    "                    elif any(y_col in col_lower for y_col in column_mappings['y']):\n",
    "                        y = row[col]\n",
    "                    elif any(w_col in col_lower for w_col in column_mappings['width']):\n",
    "                        w = row[col]\n",
    "                    elif any(h_col in col_lower for h_col in column_mappings['height']):\n",
    "                        h = row[col]\n",
    "                \n",
    "                if all(v is not None for v in [x, y, w, h]):\n",
    "                    bbox = [x, y, w, h]\n",
    "            \n",
    "            # Extract behavior\n",
    "            behavior = None\n",
    "            behavior_encoded = 0\n",
    "            \n",
    "            for col in df.columns:\n",
    "                if any(beh_col in col.lower() for beh_col in column_mappings['behavior']):\n",
    "                    behavior = row[col]\n",
    "                    break\n",
    "            \n",
    "            for col in df.columns:\n",
    "                if any(beh_col in col.lower() for beh_col in column_mappings['behavior_encoded']):\n",
    "                    behavior_encoded = row[col]\n",
    "                    break\n",
    "            \n",
    "            # Create standardized entry\n",
    "            if video_path and bbox:\n",
    "                standardized_data.append({\n",
    "                    'video_path': str(video_path),\n",
    "                    'video_id': video_id or f\"video_{idx}\",\n",
    "                    'frame_num': int(frame_num),\n",
    "                    'bbox': bbox,\n",
    "                    'behavior': behavior or \"pig_behavior\",\n",
    "                    'behavior_encoded': int(behavior_encoded)\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if standardized_data:\n",
    "        result_df = pd.DataFrame(standardized_data)\n",
    "        print(f\"Successfully standardized {len(result_df)} annotations\")\n",
    "        return result_df\n",
    "    else:\n",
    "        print(\"Could not standardize any annotations\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def create_basic_annotations_from_videos(video_files):\n",
    "    \"\"\"Create basic annotations from video files (for testing)\"\"\"\n",
    "    print(\"Creating basic annotations from video files...\")\n",
    "    \n",
    "    annotations = []\n",
    "    \n",
    "    for video_file in video_files[:10]:  # Limit to first 10 videos\n",
    "        try:\n",
    "            # Get video info\n",
    "            cap = cv2.VideoCapture(str(video_file))\n",
    "            if not cap.isOpened():\n",
    "                continue\n",
    "                \n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            cap.release()\n",
    "            \n",
    "            # Create annotations for every 30th frame (sample)\n",
    "            for frame_num in range(0, min(total_frames, 3000), 30):\n",
    "                annotations.append({\n",
    "                    'video_path': str(video_file),\n",
    "                    'video_id': video_file.stem,\n",
    "                    'frame_num': frame_num,\n",
    "                    'bbox': [100, 100, 200, 200],  # Dummy bbox - you'll need real annotations\n",
    "                    'behavior': 'pig_behavior',\n",
    "                    'behavior_encoded': 0\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if annotations:\n",
    "        print(f\"Created {len(annotations)} basic annotations\")\n",
    "        print(\"‚ö†Ô∏è  WARNING: Using dummy bounding boxes! You need real annotations for proper training.\")\n",
    "        return pd.DataFrame(annotations)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def elite_mini_pipeline(data_root):\n",
    "    \"\"\"Optimized pipeline for HP Elite Mini 800 G9\"\"\"\n",
    "    print(\"Starting Elite Mini 800 G9 optimized pipeline...\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data information...\")\n",
    "    data_info = load_annotated_data_info()\n",
    "    \n",
    "    if data_info.empty:\n",
    "        print(\"No data found - please implement load_annotated_data_info()\")\n",
    "        return None\n",
    "    \n",
    "    # Can handle more data\n",
    "    data_info = data_info.head(ELITE_MINI_CONFIG['max_frames'])\n",
    "    print(f\"Using {len(data_info)} frames for training\")\n",
    "    \n",
    "    # Prepare dataset\n",
    "    print(\"Preparing dataset...\")\n",
    "    prepare_standard_dataset(data_info, 'elite_mini_dataset')\n",
    "    \n",
    "    # Initialize detector\n",
    "    detector = EliteMiniYOLODetector()\n",
    "    \n",
    "    # Train with optimizations\n",
    "    print(\"Training detector (estimated 2-4 hours)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = detector.train_optimized('elite_mini_dataset/data.yaml')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time/3600:.2f} hours\")\n",
    "    \n",
    "    return detector\n",
    "\n",
    "def test_elite_mini_video(detector, video_path):\n",
    "    \"\"\"Test video processing on Elite Mini\"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Video file {video_path} does not exist\")\n",
    "        return\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Cannot open video {video_path}\")\n",
    "        return\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Processing video with {total_frames} frames\")\n",
    "    \n",
    "    frame_count = 0\n",
    "    processed_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        # Process every 5th frame (reasonable for 12-thread CPU)\n",
    "        if frame_count % ELITE_MINI_CONFIG['frame_skip'] == 0:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            detections = detector.detect_optimized(frame)\n",
    "            \n",
    "            process_time = time.time() - start_time\n",
    "            processed_count += 1\n",
    "            \n",
    "            print(f\"Frame {frame_count}: {len(detections)} detections, \"\n",
    "                  f\"Processing time: {process_time:.2f}s\")\n",
    "            \n",
    "            # Less frequent memory cleanup\n",
    "            if processed_count % 100 == 0:\n",
    "                gc.collect()\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Processed {processed_count} frames out of {total_frames}\")\n",
    "    print(f\"Average processing speed: {processed_count/total_frames*100:.1f}% of frames\")\n",
    "\n",
    "# Enhanced memory monitoring for 16GB system\n",
    "def monitor_memory():\n",
    "    \"\"\"Monitor detailed memory usage\"\"\"\n",
    "    import psutil\n",
    "    \n",
    "    # Overall system memory\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Total RAM: {memory.total / 1024**3:.1f} GB\")\n",
    "    print(f\"Available RAM: {memory.available / 1024**3:.1f} GB\")\n",
    "    print(f\"Used RAM: {memory.used / 1024**3:.1f} GB ({memory.percent:.1f}%)\")\n",
    "    \n",
    "    # Process-specific memory\n",
    "    process = psutil.Process()\n",
    "    process_memory = process.memory_info()\n",
    "    print(f\"Process RSS: {process_memory.rss / 1024**3:.2f} GB\")\n",
    "    print(f\"Process VMS: {process_memory.vms / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Check if we're approaching memory limits\n",
    "    if memory.percent > 85:\n",
    "        print(\"‚ö†Ô∏è  WARNING: High memory usage detected!\")\n",
    "        print(\"Consider reducing batch_size or max_frames\")\n",
    "    elif memory.percent > 70:\n",
    "        print(\"‚ÑπÔ∏è  Memory usage is getting high, monitoring recommended\")\n",
    "    else:\n",
    "        print(\"‚úÖ Memory usage is healthy\")\n",
    "\n",
    "def optimize_for_16gb():\n",
    "    \"\"\"Suggest optimizations based on current memory usage\"\"\"\n",
    "    import psutil\n",
    "    memory = psutil.virtual_memory()\n",
    "    \n",
    "    if memory.available < 4 * 1024**3:  # Less than 4GB available\n",
    "        print(\"üîß Reducing batch size for low available memory\")\n",
    "        ELITE_MINI_CONFIG['batch_size'] = 2\n",
    "        ELITE_MINI_CONFIG['max_frames'] = 4000\n",
    "        ELITE_MINI_CONFIG['workers'] = 4\n",
    "    elif memory.available < 6 * 1024**3:  # Less than 6GB available\n",
    "        print(\"üîß Using moderate settings for medium available memory\")\n",
    "        ELITE_MINI_CONFIG['batch_size'] = 4\n",
    "        ELITE_MINI_CONFIG['max_frames'] = 6000\n",
    "        ELITE_MINI_CONFIG['workers'] = 6\n",
    "    else:\n",
    "        print(\"‚úÖ Using full settings for high available memory\")\n",
    "        # Use default settings from config\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Monitor initial memory and optimize settings\n",
    "    print(\"=== Initial System Status ===\")\n",
    "    monitor_memory()\n",
    "    optimize_for_16gb()\n",
    "    \n",
    "    print(\"\\n=== Starting Training Pipeline ===\")\n",
    "    \n",
    "    # Run Elite Mini optimized pipeline\n",
    "    detector = elite_mini_pipeline(\"C:/Users/2955352g/Desktop/pig_data_edinburgh\")\n",
    "    \n",
    "    if detector:\n",
    "        print(\"Training completed successfully!\")\n",
    "        \n",
    "        # Monitor memory after training\n",
    "        print(\"\\n=== Post-Training Memory Status ===\")\n",
    "        monitor_memory()\n",
    "        \n",
    "        # Test on video\n",
    "        test_video = \"test_video.mp4\"  # Adjust path\n",
    "        if os.path.exists(test_video):\n",
    "            test_elite_mini_video(detector, test_video)\n",
    "    \n",
    "    # Final memory check\n",
    "    print(\"\\n=== Final Memory Status ===\")\n",
    "    monitor_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kj\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
