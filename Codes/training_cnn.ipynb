{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"C:/Users/2955352g/Desktop/pig_data_edinburgh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnotatedDataGenerator(Sequence):\n",
    "    \"\"\"Data generator for annotated frames to avoid memory issues\"\"\"\n",
    "    \n",
    "    def __init__(self, data_info, batch_size=8, target_size=(224, 224), shuffle=True):\n",
    "        self.data_info = data_info\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(data_info))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data_info) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_data = [self.data_info.iloc[i] for i in batch_indices]\n",
    "        \n",
    "        X, y = self._load_batch(batch_data)\n",
    "        return X, y\n",
    "    \n",
    "    def _load_batch(self, batch_data):\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for data in batch_data:\n",
    "            try:\n",
    "                # Load frame from video\n",
    "                cap = cv2.VideoCapture(str(data['video_path']))\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, data['frame_num'])\n",
    "                ret, frame = cap.read()\n",
    "                cap.release()\n",
    "                \n",
    "                if ret:\n",
    "                    # Resize frame\n",
    "                    frame = cv2.resize(frame, self.target_size)\n",
    "                    frame = frame.astype(np.float32) / 255.0  # Normalize\n",
    "                    X.append(frame)\n",
    "                    y.append(data['behavior_encoded'])\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading frame: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotated_data_info():\n",
    "    \"\"\"Load annotated data information without loading frames into memory\"\"\"\n",
    "    data_info = []\n",
    "    \n",
    "    # Check if annotated directory exists\n",
    "    annotated_dir = DATA_ROOT / \"annotated\"\n",
    "    if not annotated_dir.exists():\n",
    "        print(f\"Warning: Annotated directory {annotated_dir} does not exist\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    json_files = list(annotated_dir.rglob(\"output.json\"))\n",
    "    if not json_files:\n",
    "        print(\"No output.json files found in annotated directory\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(json_files)} annotation files\")\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file) as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            video_path = json_file.parent / \"color.mp4\"\n",
    "            if not video_path.exists():\n",
    "                print(f\"Warning: Video file {video_path} does not exist\")\n",
    "                continue\n",
    "            \n",
    "            # Quick check if video can be opened\n",
    "            cap = cv2.VideoCapture(str(video_path))\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Warning: Cannot open video {video_path}\")\n",
    "                continue\n",
    "            \n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            cap.release()\n",
    "            \n",
    "            print(f\"Processing {video_path} with {total_frames} frames\")\n",
    "            \n",
    "            for obj in data.get('objects', []):\n",
    "                for frame_data in obj.get('frames', []):\n",
    "                    frame_num = frame_data.get('frameNumber', 0)\n",
    "                    if frame_num >= total_frames:\n",
    "                        continue\n",
    "                    \n",
    "                    # Handle different bbox formats\n",
    "                    bbox = frame_data.get('bbox', [0, 0, 100, 100])\n",
    "                    \n",
    "                    # Debug: Print first few bbox formats\n",
    "                    if len(data_info) < 3:\n",
    "                        print(f\"  Sample bbox: {bbox}, type: {type(bbox)}\")\n",
    "                    \n",
    "                    # Normalize bbox format\n",
    "                    try:\n",
    "                        if isinstance(bbox, str):\n",
    "                            import ast\n",
    "                            bbox = ast.literal_eval(bbox)\n",
    "                        \n",
    "                        if isinstance(bbox, dict):\n",
    "                            # Convert dict format to list [x, y, width, height]\n",
    "                            x = bbox.get('x', 0)\n",
    "                            y = bbox.get('y', 0)\n",
    "                            w = bbox.get('width', bbox.get('w', 100))\n",
    "                            h = bbox.get('height', bbox.get('h', 100))\n",
    "                            bbox = [x, y, w, h]\n",
    "                        \n",
    "                        # Ensure bbox is a list with 4 elements\n",
    "                        if not isinstance(bbox, (list, tuple)) or len(bbox) != 4:\n",
    "                            bbox = [0, 0, 100, 100]\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing bbox {bbox}: {e}\")\n",
    "                        bbox = [0, 0, 100, 100]\n",
    "                    \n",
    "                    data_info.append({\n",
    "                        'video_path': str(video_path),\n",
    "                        'behavior': frame_data.get('behaviour', 'unknown'),\n",
    "                        'bbox': bbox,\n",
    "                        'video_id': json_file.parent.name,\n",
    "                        'date': json_file.parent.parent.name,\n",
    "                        'frame_num': frame_num\n",
    "                    })\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {json_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Found {len(data_info)} annotated frames\")\n",
    "    return pd.DataFrame(data_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_frames(data_info, sample_size=100):\n",
    "    \"\"\"Load a small sample of frames for quick testing\"\"\"\n",
    "    if len(data_info) > sample_size:\n",
    "        sample_info = data_info.sample(n=sample_size).reset_index(drop=True)\n",
    "    else:\n",
    "        sample_info = data_info.copy()\n",
    "    \n",
    "    frames = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for idx, row in sample_info.iterrows():\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(str(row['video_path']))\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, row['frame_num'])\n",
    "            ret, frame = cap.read()\n",
    "            cap.release()\n",
    "            \n",
    "            if ret:\n",
    "                frame = cv2.resize(frame, (224, 224))\n",
    "                frame = frame.astype(np.float32) / 255.0\n",
    "                frames.append(frame)\n",
    "                valid_indices.append(idx)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading frame {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return np.array(frames), sample_info.iloc[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_detector(input_shape=(224, 224, 3)):\n",
    "    \"\"\"Simplified detector for memory efficiency\"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Simple CNN for detection\n",
    "    x = Conv2D(32, (3,3), strides=(2,2), padding='same', activation='relu')(inputs)\n",
    "    x = Conv2D(64, (3,3), strides=(2,2), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(128, (3,3), strides=(2,2), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(256, (3,3), strides=(2,2), padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Global average pooling\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Output layer for bounding box regression\n",
    "    output = Dense(4, activation='sigmoid')(x)  # [x, y, w, h] normalized\n",
    "    \n",
    "    return Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_detector(data_info):\n",
    "    \"\"\"Train detector using data generator\"\"\"\n",
    "    if data_info.empty:\n",
    "        print(\"No annotated data available for training detector\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Training detector on {len(data_info)} samples\")\n",
    "    \n",
    "    # Debug: Check bbox format\n",
    "    print(\"Sample bbox data:\")\n",
    "    for i, row in data_info.head(3).iterrows():\n",
    "        print(f\"  Row {i}: bbox = {row['bbox']}, type = {type(row['bbox'])}\")\n",
    "    \n",
    "    # Use only a sample for detector training to save memory\n",
    "    sample_size = min(1000, len(data_info))\n",
    "    sample_frames, sample_info = load_sample_frames(data_info, sample_size)\n",
    "    \n",
    "    if len(sample_frames) == 0:\n",
    "        print(\"No frames could be loaded for detector training\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare normalized bounding boxes\n",
    "    y_train = []\n",
    "    for _, row in sample_info.iterrows():\n",
    "        bbox = row['bbox']\n",
    "        \n",
    "        # Handle different bbox formats\n",
    "        try:\n",
    "            if isinstance(bbox, str):\n",
    "                # If bbox is a string, try to parse it\n",
    "                import ast\n",
    "                bbox = ast.literal_eval(bbox)\n",
    "            \n",
    "            if isinstance(bbox, dict):\n",
    "                # If bbox is a dictionary, extract values\n",
    "                x = bbox.get('x', 0)\n",
    "                y = bbox.get('y', 0)\n",
    "                w = bbox.get('width', bbox.get('w', 100))\n",
    "                h = bbox.get('height', bbox.get('h', 100))\n",
    "                bbox = [x, y, w, h]\n",
    "            \n",
    "            # Ensure bbox is a list/array with 4 elements\n",
    "            if not isinstance(bbox, (list, tuple, np.ndarray)) or len(bbox) != 4:\n",
    "                print(f\"Invalid bbox format: {bbox}, using default\")\n",
    "                bbox = [0, 0, 100, 100]\n",
    "            \n",
    "            # Normalize bbox (assuming original frame size is 1280x720)\n",
    "            x_norm = float(bbox[0]) / 1280.0\n",
    "            y_norm = float(bbox[1]) / 720.0\n",
    "            w_norm = float(bbox[2]) / 1280.0\n",
    "            h_norm = float(bbox[3]) / 720.0\n",
    "            \n",
    "            # Clamp values to [0, 1]\n",
    "            x_norm = max(0, min(1, x_norm))\n",
    "            y_norm = max(0, min(1, y_norm))\n",
    "            w_norm = max(0, min(1, w_norm))\n",
    "            h_norm = max(0, min(1, h_norm))\n",
    "            \n",
    "            y_train.append([x_norm, y_norm, w_norm, h_norm])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing bbox {bbox}: {e}\")\n",
    "            # Use default normalized bbox\n",
    "            y_train.append([0.1, 0.1, 0.2, 0.2])\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    # Build and compile model\n",
    "    model = build_simple_detector()\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    print(\"Training detector...\")\n",
    "    history = model.fit(\n",
    "        sample_frames, y_train,\n",
    "        epochs=10,\n",
    "        batch_size=16,\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_behavior_classifier(input_shape=(224,224,3), num_classes=5):\n",
    "    \"\"\"Lightweight behavior classifier\"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Use a smaller CNN instead of ResNet50 for memory efficiency\n",
    "    x = Conv2D(32, (3,3), strides=(2,2), padding='same', activation='relu')(inputs)\n",
    "    x = Conv2D(64, (3,3), strides=(2,2), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(128, (3,3), strides=(2,2), padding='same', activation='relu')(x)\n",
    "    \n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_behavior_classifier(data_info):\n",
    "    \"\"\"Train behavior classifier using data generator\"\"\"\n",
    "    if data_info.empty:\n",
    "        print(\"No annotated data available for training behavior classifier\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Training behavior classifier on {len(data_info)} samples\")\n",
    "    \n",
    "    # Encode behaviors\n",
    "    unique_behaviors = data_info['behavior'].unique()\n",
    "    behavior_to_idx = {behavior: idx for idx, behavior in enumerate(unique_behaviors)}\n",
    "    \n",
    "    data_info_copy = data_info.copy()\n",
    "    data_info_copy['behavior_encoded'] = data_info_copy['behavior'].map(behavior_to_idx)\n",
    "    \n",
    "    # Convert to one-hot encoding\n",
    "    num_classes = len(unique_behaviors)\n",
    "    encoded_behaviors = []\n",
    "    for idx in data_info_copy['behavior_encoded']:\n",
    "        one_hot = np.zeros(num_classes)\n",
    "        one_hot[idx] = 1\n",
    "        encoded_behaviors.append(one_hot)\n",
    "    \n",
    "    data_info_copy['behavior_encoded'] = encoded_behaviors\n",
    "    \n",
    "    print(f\"Found {num_classes} behavior classes: {list(unique_behaviors)}\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_behavior_classifier(num_classes=num_classes)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Create data generator\n",
    "    train_generator = AnnotatedDataGenerator(\n",
    "        data_info_copy, \n",
    "        batch_size=16, \n",
    "        target_size=(224, 224)\n",
    "    )\n",
    "    \n",
    "    print(\"Training behavior classifier...\")\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=5,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, behavior_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PigTracker:\n",
    "    def __init__(self, max_age=8):\n",
    "        self.trackers = defaultdict(dict)\n",
    "        self.max_age = max_age\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def update(self, frame, detections):\n",
    "        \"\"\"Update trackers with new detections\"\"\"\n",
    "        active_trackers = {}\n",
    "        \n",
    "        # Update existing trackers\n",
    "        for tid in list(self.trackers.keys()):\n",
    "            tracker = self.trackers[tid]['tracker']\n",
    "            bbox = self.trackers[tid]['bbox']\n",
    "            \n",
    "            # Update tracker with new frame\n",
    "            success, new_bbox = tracker.update(frame)\n",
    "            \n",
    "            if success:\n",
    "                active_trackers[tid] = {\n",
    "                    'bbox': new_bbox,\n",
    "                    'tracker': tracker,\n",
    "                    'age': 0\n",
    "                }\n",
    "            elif self.trackers[tid]['age'] < self.max_age:\n",
    "                active_trackers[tid] = {\n",
    "                    'bbox': bbox,\n",
    "                    'tracker': tracker,\n",
    "                    'age': self.trackers[tid]['age'] + 1\n",
    "                }\n",
    "        \n",
    "        # Create new trackers for unmatched detections\n",
    "        for det in detections:\n",
    "            matched = False\n",
    "            for tid in active_trackers:\n",
    "                if self._iou(det['bbox'], active_trackers[tid]['bbox']) > 0.4:\n",
    "                    matched = True\n",
    "                    break\n",
    "            \n",
    "            if not matched:\n",
    "                # Use CSRT tracker\n",
    "                tracker = cv2.TrackerCSRT_create()\n",
    "                tracker.init(frame, tuple(det['bbox']))\n",
    "                active_trackers[self.next_id] = {\n",
    "                    'bbox': det['bbox'],\n",
    "                    'tracker': tracker,\n",
    "                    'age': 0\n",
    "                }\n",
    "                self.next_id += 1\n",
    "        \n",
    "        self.trackers = active_trackers\n",
    "        return self.trackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _iou(self, box1, box2):\n",
    "        \"\"\"Calculate Intersection over Union\"\"\"\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[0]+box1[2], box2[0]+box2[2])\n",
    "        y2 = min(box1[1]+box1[3], box2[1]+box2[3])\n",
    "        \n",
    "        inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "        box1_area = box1[2] * box1[3]\n",
    "        box2_area = box2[2] * box2[3]\n",
    "        \n",
    "        return inter_area / float(box1_area + box2_area - inter_area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_pipeline():\n",
    "    print(\"Starting pig behavior detection pipeline...\")\n",
    "    \n",
    "    # Check if data directory exists\n",
    "    if not DATA_ROOT.exists():\n",
    "        print(f\"Error: Data directory {DATA_ROOT} does not exist\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(\"Loading data information...\")\n",
    "    data_info = load_annotated_data_info()\n",
    "    \n",
    "    if data_info.empty:\n",
    "        print(\"No annotated data found. Cannot proceed with training.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(\"Training detector...\")\n",
    "    detector = train_detector(data_info)\n",
    "    \n",
    "    print(\"Initializing tracker...\")\n",
    "    tracker = PigTracker()\n",
    "    \n",
    "    print(\"Training behavior classifier...\")\n",
    "    behavior_model, behavior_mapping = train_behavior_classifier(data_info)\n",
    "    \n",
    "    print(\"Pipeline training complete!\")\n",
    "    return detector, tracker, behavior_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_video(detector, behavior_model, video_path):\n",
    "    \"\"\"Test the trained models on a single video\"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Video file {video_path} does not exist\")\n",
    "        return\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Cannot open video {video_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    \n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        if frame_count % 30 == 0:  # Process every 30th frame\n",
    "            # Resize frame for models\n",
    "            frame_resized = cv2.resize(frame, (224, 224))\n",
    "            frame_norm = frame_resized.astype(np.float32) / 255.0\n",
    "            frame_batch = np.expand_dims(frame_norm, axis=0)\n",
    "            \n",
    "            # Get predictions\n",
    "            if detector:\n",
    "                bbox_pred = detector.predict(frame_batch, verbose=0)\n",
    "                print(f\"Frame {frame_count}: Detected bbox: {bbox_pred[0]}\")\n",
    "            \n",
    "            if behavior_model:\n",
    "                behavior_pred = behavior_model.predict(frame_batch, verbose=0)\n",
    "                behavior_class = np.argmax(behavior_pred[0])\n",
    "                confidence = np.max(behavior_pred[0])\n",
    "                print(f\"Frame {frame_count}: Behavior class: {behavior_class}, Confidence: {confidence:.3f}\")\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Processed {frame_count} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pig behavior detection pipeline...\n",
      "Loading data information...\n",
      "Found 12 annotation files\n",
      "Processing C:\\Users\\2955352g\\Desktop\\pig_data_edinburgh\\annotated\\2019_11_05\\000002\\color.mp4 with 1800 frames\n",
      "  Sample bbox: {'x': 196, 'y': 299, 'width': 240, 'height': 121}, type: <class 'dict'>\n",
      "  Sample bbox: {'x': 196, 'y': 290, 'width': 261, 'height': 122}, type: <class 'dict'>\n",
      "  Sample bbox: {'x': 196, 'y': 283, 'width': 251, 'height': 190}, type: <class 'dict'>\n",
      "Processing C:\\Users\\2955352g\\Desktop\\pig_data_edinburgh\\annotated\\2019_11_05\\000009\\color.mp4 with 1800 frames\n",
      "Processing C:\\Users\\2955352g\\Desktop\\pig_data_edinburgh\\annotated\\2019_11_11\\000016\\color.mp4 with 1800 frames\n",
      "Processing C:\\Users\\2955352g\\Desktop\\pig_data_edinburgh\\annotated\\2019_11_11\\000028\\color.mp4 with 1800 frames\n",
      "Processing C:\\Users\\2955352g\\Desktop\\pig_data_edinburgh\\annotated\\2019_11_11\\000036\\color.mp4 with 1800 frames\n",
      "Processing C:\\Users\\2955352g\\Desktop\\pig_data_edinburgh\\annotated\\2019_11_15\\000033\\color.mp4 with 1800 frames\n",
      "Processing C:\\Users\\2955352g\\Desktop\\pig_data_edinburgh\\annotated\\2019_11_22\\000010\\color.mp4 with 1800 frames\n",
      "Processing C:\\Users\\2955352g\\Desktop\\pig_data_edinburgh\\annotated\\2019_11_28\\000113\\color.mp4 with 1800 frames\n",
      "Processing C:\\Users\\2955352g\\Desktop\\pig_data_edinburgh\\annotated\\2019_12_02\\000005\\color.mp4 with 1800 frames\n",
      "Processing C:\\Users\\2955352g\\Desktop\\pig_data_edinburgh\\annotated\\2019_12_02\\000208\\color.mp4 with 1800 frames\n",
      "Processing C:\\Users\\2955352g\\Desktop\\pig_data_edinburgh\\annotated\\2019_12_10\\000060\\color.mp4 with 1800 frames\n",
      "Processing C:\\Users\\2955352g\\Desktop\\pig_data_edinburgh\\annotated\\2019_12_10\\000078\\color.mp4 with 1800 frames\n",
      "Found 12646 annotated frames\n",
      "Training detector...\n",
      "Training detector on 12646 samples\n",
      "Sample bbox data:\n",
      "  Row 0: bbox = [196, 299, 240, 121], type = <class 'list'>\n",
      "  Row 1: bbox = [196, 290, 261, 122], type = <class 'list'>\n",
      "  Row 2: bbox = [196, 283, 251, 190], type = <class 'list'>\n",
      "Training detector...\n",
      "Epoch 1/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 101ms/step - loss: 0.0332 - mae: 0.1440 - val_loss: 0.0233 - val_mae: 0.1152\n",
      "Epoch 2/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - loss: 0.0226 - mae: 0.1127 - val_loss: 0.0218 - val_mae: 0.1119\n",
      "Epoch 3/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - loss: 0.0226 - mae: 0.1146 - val_loss: 0.0227 - val_mae: 0.1137\n",
      "Epoch 4/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 116ms/step - loss: 0.0228 - mae: 0.1144 - val_loss: 0.0213 - val_mae: 0.1119\n",
      "Epoch 5/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - loss: 0.0234 - mae: 0.1165 - val_loss: 0.0215 - val_mae: 0.1116\n",
      "Epoch 6/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - loss: 0.0238 - mae: 0.1172 - val_loss: 0.0229 - val_mae: 0.1156\n",
      "Epoch 7/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - loss: 0.0231 - mae: 0.1145 - val_loss: 0.0214 - val_mae: 0.1117\n",
      "Epoch 8/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - loss: 0.0236 - mae: 0.1167 - val_loss: 0.0221 - val_mae: 0.1144\n",
      "Epoch 9/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step - loss: 0.0231 - mae: 0.1148 - val_loss: 0.0228 - val_mae: 0.1179\n",
      "Epoch 10/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 124ms/step - loss: 0.0229 - mae: 0.1152 - val_loss: 0.0215 - val_mae: 0.1120\n",
      "Initializing tracker...\n",
      "Training behavior classifier...\n",
      "Training behavior classifier on 12646 samples\n",
      "Found 16 behavior classes: ['investigating', 'nose-to-nose', 'walk', 'drink', 'standing', 'run', 'nose-poke-elsewhere', 'playwithtoy', 'jumpontopof', 'other', 'lying', 'sitting', 'fight', 'eat', 'sleep', 'chase']\n",
      "Training behavior classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2955352g\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m791/791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6106s\u001b[0m 8s/step - accuracy: 0.3316 - loss: 2.0345\n",
      "Epoch 2/5\n",
      "\u001b[1m791/791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6137s\u001b[0m 8s/step - accuracy: 0.3421 - loss: 1.8600\n",
      "Epoch 3/5\n",
      "\u001b[1m791/791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15713s\u001b[0m 20s/step - accuracy: 0.3497 - loss: 1.7574\n",
      "Epoch 4/5\n",
      "\u001b[1m791/791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6008s\u001b[0m 8s/step - accuracy: 0.3540 - loss: 1.7346\n",
      "Epoch 5/5\n",
      "\u001b[1m791/791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5771s\u001b[0m 7s/step - accuracy: 0.3664 - loss: 1.6904\n",
      "Pipeline training complete!\n",
      "Detection model trained successfully\n",
      "Behavior classification model trained successfully\n",
      "Tracker initialized successfully\n",
      "Processing video: C:\\Users\\2955352g\\Desktop\\pig_data_edinburgh\\annotated\\test_video.mp4\n",
      "Frame 30: Detected bbox: [0.4492801  0.4113723  0.16241232 0.27503106]\n",
      "Frame 30: Behavior class: 0, Confidence: 0.341\n",
      "Frame 60: Detected bbox: [0.44819766 0.40995285 0.15757634 0.27056196]\n",
      "Frame 60: Behavior class: 0, Confidence: 0.332\n",
      "Frame 90: Detected bbox: [0.44944987 0.41172624 0.16300143 0.2757967 ]\n",
      "Frame 90: Behavior class: 0, Confidence: 0.329\n",
      "Frame 120: Detected bbox: [0.44907835 0.41140833 0.1618909  0.27451026]\n",
      "Frame 120: Behavior class: 0, Confidence: 0.325\n",
      "Frame 150: Detected bbox: [0.4484383  0.4099105  0.15764967 0.2711492 ]\n",
      "Frame 150: Behavior class: 0, Confidence: 0.336\n",
      "Frame 180: Detected bbox: [0.4488627  0.41168416 0.16284752 0.27500552]\n",
      "Frame 180: Behavior class: 0, Confidence: 0.310\n",
      "Frame 210: Detected bbox: [0.4489209  0.41153285 0.16266726 0.27539715]\n",
      "Frame 210: Behavior class: 0, Confidence: 0.324\n",
      "Frame 240: Detected bbox: [0.4486431  0.41104352 0.1609173  0.27344215]\n",
      "Frame 240: Behavior class: 0, Confidence: 0.336\n",
      "Frame 270: Detected bbox: [0.44822517 0.41001815 0.15833637 0.27143234]\n",
      "Frame 270: Behavior class: 0, Confidence: 0.346\n",
      "Frame 300: Detected bbox: [0.44858477 0.4108245  0.16035333 0.27283603]\n",
      "Frame 300: Behavior class: 0, Confidence: 0.337\n",
      "Frame 330: Detected bbox: [0.4484129  0.4103316  0.15963    0.27274942]\n",
      "Frame 330: Behavior class: 0, Confidence: 0.354\n",
      "Frame 360: Detected bbox: [0.44793358 0.4097267  0.1572785  0.27043885]\n",
      "Frame 360: Behavior class: 0, Confidence: 0.348\n",
      "Frame 390: Detected bbox: [0.44832924 0.41018748 0.15887757 0.27207455]\n",
      "Frame 390: Behavior class: 0, Confidence: 0.327\n",
      "Frame 420: Detected bbox: [0.44861332 0.41099328 0.16121005 0.27374816]\n",
      "Frame 420: Behavior class: 0, Confidence: 0.368\n",
      "Frame 450: Detected bbox: [0.44853467 0.41075855 0.16059864 0.27310103]\n",
      "Frame 450: Behavior class: 0, Confidence: 0.341\n",
      "Frame 480: Detected bbox: [0.4489205  0.41095367 0.16119196 0.27412498]\n",
      "Frame 480: Behavior class: 0, Confidence: 0.364\n",
      "Frame 510: Detected bbox: [0.44856295 0.41063613 0.1600534  0.27294812]\n",
      "Frame 510: Behavior class: 0, Confidence: 0.343\n",
      "Frame 540: Detected bbox: [0.44884822 0.41114798 0.16162501 0.27411368]\n",
      "Frame 540: Behavior class: 0, Confidence: 0.328\n",
      "Frame 570: Detected bbox: [0.44868788 0.41095936 0.1611244  0.27379185]\n",
      "Frame 570: Behavior class: 0, Confidence: 0.346\n",
      "Frame 600: Detected bbox: [0.44913998 0.41147068 0.16269198 0.27511495]\n",
      "Frame 600: Behavior class: 0, Confidence: 0.363\n",
      "Frame 630: Detected bbox: [0.44845143 0.4104695  0.159741   0.27277696]\n",
      "Frame 630: Behavior class: 0, Confidence: 0.364\n",
      "Frame 660: Detected bbox: [0.44832343 0.41034397 0.15886766 0.27174026]\n",
      "Frame 660: Behavior class: 0, Confidence: 0.344\n",
      "Frame 690: Detected bbox: [0.4485323  0.41060847 0.16032404 0.2732572 ]\n",
      "Frame 690: Behavior class: 0, Confidence: 0.367\n",
      "Frame 720: Detected bbox: [0.44835636 0.41035032 0.15951169 0.2723251 ]\n",
      "Frame 720: Behavior class: 0, Confidence: 0.351\n",
      "Frame 750: Detected bbox: [0.44891113 0.41096473 0.16145547 0.27434707]\n",
      "Frame 750: Behavior class: 0, Confidence: 0.349\n",
      "Frame 780: Detected bbox: [0.44831246 0.41043597 0.15945265 0.27234283]\n",
      "Frame 780: Behavior class: 0, Confidence: 0.348\n",
      "Frame 810: Detected bbox: [0.44902676 0.41123897 0.16140917 0.2742751 ]\n",
      "Frame 810: Behavior class: 0, Confidence: 0.349\n",
      "Frame 840: Detected bbox: [0.44801793 0.40975583 0.15755239 0.2708883 ]\n",
      "Frame 840: Behavior class: 0, Confidence: 0.328\n",
      "Frame 870: Detected bbox: [0.44906142 0.41173598 0.16362664 0.27609053]\n",
      "Frame 870: Behavior class: 0, Confidence: 0.357\n",
      "Frame 900: Detected bbox: [0.448353   0.41083488 0.16063169 0.27344835]\n",
      "Frame 900: Behavior class: 0, Confidence: 0.377\n",
      "Frame 930: Detected bbox: [0.44870165 0.4114347  0.16271646 0.2748854 ]\n",
      "Frame 930: Behavior class: 0, Confidence: 0.355\n",
      "Frame 960: Detected bbox: [0.4487714  0.4110085  0.16152143 0.27410346]\n",
      "Frame 960: Behavior class: 0, Confidence: 0.332\n",
      "Frame 990: Detected bbox: [0.44821182 0.410094   0.15822491 0.27105275]\n",
      "Frame 990: Behavior class: 0, Confidence: 0.342\n",
      "Frame 1020: Detected bbox: [0.4482271  0.41028783 0.15923935 0.2721095 ]\n",
      "Frame 1020: Behavior class: 0, Confidence: 0.356\n",
      "Frame 1050: Detected bbox: [0.44850224 0.4104093  0.15925726 0.27227297]\n",
      "Frame 1050: Behavior class: 0, Confidence: 0.360\n",
      "Frame 1080: Detected bbox: [0.44800165 0.4098429  0.15817767 0.27156422]\n",
      "Frame 1080: Behavior class: 0, Confidence: 0.365\n",
      "Frame 1110: Detected bbox: [0.44844943 0.4100487  0.15834334 0.2716657 ]\n",
      "Frame 1110: Behavior class: 0, Confidence: 0.356\n",
      "Frame 1140: Detected bbox: [0.44813848 0.40986258 0.157813   0.27099138]\n",
      "Frame 1140: Behavior class: 0, Confidence: 0.345\n",
      "Frame 1170: Detected bbox: [0.4479624  0.4098163  0.15769073 0.270639  ]\n",
      "Frame 1170: Behavior class: 0, Confidence: 0.343\n",
      "Frame 1200: Detected bbox: [0.4481275  0.40993154 0.15805484 0.2710985 ]\n",
      "Frame 1200: Behavior class: 0, Confidence: 0.349\n",
      "Frame 1230: Detected bbox: [0.44825774 0.41003028 0.158214   0.27130866]\n",
      "Frame 1230: Behavior class: 0, Confidence: 0.371\n",
      "Frame 1260: Detected bbox: [0.44856453 0.41038582 0.15896405 0.27191436]\n",
      "Frame 1260: Behavior class: 0, Confidence: 0.331\n",
      "Frame 1290: Detected bbox: [0.44844347 0.41032514 0.15910485 0.27216044]\n",
      "Frame 1290: Behavior class: 0, Confidence: 0.330\n",
      "Frame 1320: Detected bbox: [0.44927973 0.41122085 0.161315   0.27415535]\n",
      "Frame 1320: Behavior class: 0, Confidence: 0.332\n",
      "Frame 1350: Detected bbox: [0.44880673 0.4104803  0.15952061 0.272786  ]\n",
      "Frame 1350: Behavior class: 0, Confidence: 0.366\n",
      "Frame 1380: Detected bbox: [0.44917017 0.4113949  0.16199489 0.2746593 ]\n",
      "Frame 1380: Behavior class: 0, Confidence: 0.361\n",
      "Frame 1410: Detected bbox: [0.44843185 0.4108366  0.16082186 0.27342293]\n",
      "Frame 1410: Behavior class: 0, Confidence: 0.318\n",
      "Frame 1440: Detected bbox: [0.44937733 0.41140237 0.16211337 0.27488083]\n",
      "Frame 1440: Behavior class: 0, Confidence: 0.346\n",
      "Frame 1470: Detected bbox: [0.4492297  0.41123274 0.16151668 0.27436903]\n",
      "Frame 1470: Behavior class: 0, Confidence: 0.334\n",
      "Frame 1500: Detected bbox: [0.44950357 0.4114662  0.16225937 0.275224  ]\n",
      "Frame 1500: Behavior class: 0, Confidence: 0.338\n",
      "Frame 1530: Detected bbox: [0.44943646 0.41174114 0.16292568 0.27595192]\n",
      "Frame 1530: Behavior class: 0, Confidence: 0.366\n",
      "Frame 1560: Detected bbox: [0.4494002  0.4116078  0.16217795 0.27480882]\n",
      "Frame 1560: Behavior class: 0, Confidence: 0.348\n",
      "Frame 1590: Detected bbox: [0.44868875 0.41025564 0.15853265 0.2718782 ]\n",
      "Frame 1590: Behavior class: 0, Confidence: 0.333\n",
      "Frame 1620: Detected bbox: [0.44932586 0.41152123 0.1629513  0.27594247]\n",
      "Frame 1620: Behavior class: 0, Confidence: 0.343\n",
      "Frame 1650: Detected bbox: [0.4483397  0.41042644 0.15970655 0.27283615]\n",
      "Frame 1650: Behavior class: 0, Confidence: 0.323\n",
      "Frame 1680: Detected bbox: [0.44835374 0.41000503 0.15785481 0.27116582]\n",
      "Frame 1680: Behavior class: 0, Confidence: 0.328\n",
      "Frame 1710: Detected bbox: [0.44816917 0.4101013  0.1583365  0.27141002]\n",
      "Frame 1710: Behavior class: 0, Confidence: 0.353\n",
      "Frame 1740: Detected bbox: [0.44824764 0.41038284 0.1593472  0.2721047 ]\n",
      "Frame 1740: Behavior class: 0, Confidence: 0.321\n",
      "Frame 1770: Detected bbox: [0.44884694 0.4107425  0.16006577 0.27311862]\n",
      "Frame 1770: Behavior class: 0, Confidence: 0.346\n",
      "Frame 1800: Detected bbox: [0.4490374  0.4111563  0.1611364  0.27404788]\n",
      "Frame 1800: Behavior class: 0, Confidence: 0.342\n",
      "Processed 1800 frames\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set memory growth for GPU if available\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    detector, tracker, behavior_model = run_full_pipeline()\n",
    "    \n",
    "    if detector is not None:\n",
    "        print(\"Detection model trained successfully\")\n",
    "    if behavior_model is not None:\n",
    "        print(\"Behavior classification model trained successfully\")\n",
    "    if tracker is not None:\n",
    "        print(\"Tracker initialized successfully\")\n",
    "    \n",
    "    # Test on a single video if models are trained\n",
    "    if detector or behavior_model:\n",
    "        test_video_path = DATA_ROOT / \"annotated\" / \"test_video.mp4\"  # Adjust path as needed\n",
    "        if test_video_path.exists():\n",
    "            test_single_video(detector, behavior_model, str(test_video_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--device DEVICE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\2955352g\\AppData\\Roaming\\jupyter\\runtime\\kernel-v39555ef68165e426e0c65bde4934e1faca2dffe1e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\2955352g\\.conda\\envs\\tf_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
